{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-11T12:53:35.622645Z",
     "iopub.status.busy": "2026-01-11T12:53:35.622077Z",
     "iopub.status.idle": "2026-01-11T12:53:40.577608Z",
     "shell.execute_reply": "2026-01-11T12:53:40.576719Z",
     "shell.execute_reply.started": "2026-01-11T12:53:35.622628Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:10:23.569945Z",
     "iopub.status.busy": "2026-01-11T14:10:23.569639Z",
     "iopub.status.idle": "2026-01-11T14:10:23.575615Z",
     "shell.execute_reply": "2026-01-11T14:10:23.574922Z",
     "shell.execute_reply.started": "2026-01-11T14:10:23.569929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import ijson\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Copier les utility_scripts depuis le dataset vers le working directory\n",
    "utility_scripts_src = '/kaggle/input/dataset/utility_scripts'\n",
    "utility_scripts_dst = '/kaggle/working/utility_scripts'\n",
    "\n",
    "if os.path.exists(utility_scripts_src):\n",
    "    if os.path.exists(utility_scripts_dst):\n",
    "        shutil.rmtree(utility_scripts_dst)\n",
    "    shutil.copytree(utility_scripts_src, utility_scripts_dst)\n",
    "    print(\"Utility scripts copiés vers /kaggle/working/utility_scripts\")\n",
    "else:\n",
    "    print(\"Utility scripts non trouvés dans /kaggle/input/dataset/utility_scripts\")\n",
    "    print(\"Assurez-vous que le dataset 'dataset' contient le dossier utility_scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything from utility_scripts\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utility_scripts.tree import tree\n",
    "from utility_scripts.taxonomy import parse_taxonomy\n",
    "from utility_scripts.mapping import build_taxa_maps\n",
    "from utility_scripts.corruption_scan import verify_image_validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T12:52:54.378118Z",
     "iopub.status.busy": "2026-01-11T12:52:54.377843Z",
     "iopub.status.idle": "2026-01-11T12:52:54.383610Z",
     "shell.execute_reply": "2026-01-11T12:52:54.382186Z",
     "shell.execute_reply.started": "2026-01-11T12:52:54.378105Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Input files\n",
    "base_folder = \"/kaggle/input/inaturalist-insects/\"\n",
    "\n",
    "public_test_folder = os.path.join(base_folder, \"public_test/public_test\")\n",
    "train_folder = os.path.join(base_folder, \"train/train\")\n",
    "train_mini_folder = os.path.join(base_folder, \"train_mini/train_mini\")\n",
    "val_folder = os.path.join(base_folder, \"val/val\")\n",
    "\n",
    "public_test_json = os.path.join(base_folder, \"public_test-json/public_test.json\")\n",
    "train_json = os.path.join(base_folder, \"train-json/train.json\")\n",
    "train_mini_json = os.path.join(base_folder, \"train_mini-json/train_mini.json\")\n",
    "val_json = os.path.join(base_folder, \"val-json/val.json\")\n",
    "\n",
    "# Output files\n",
    "working_dir = \"/kaggle/working/\"\n",
    "\n",
    "tree_file = os.path.join(working_dir, \"tree.txt\")\n",
    "hierarchy_map_file = \"/kaggle/working/hierarchy_map.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-11T09:20:26.873617Z",
     "iopub.status.busy": "2026-01-11T09:20:26.873290Z",
     "iopub.status.idle": "2026-01-11T09:20:26.936737Z",
     "shell.execute_reply": "2026-01-11T09:20:26.935802Z",
     "shell.execute_reply.started": "2026-01-11T09:20:26.873594Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "example = \"train/train/00980_Animalia_Arthropoda_Insecta_Lepidoptera_Erebidae_Arctia_virginalis/464f3a34-4c04-4eb3-afa2-6cb7444c3fa3.jpg\"\n",
    "taxonomy = parse_taxonomy(example)\n",
    "print(\"Résultat:\", taxonomy)\n",
    "validity = verify_image_validy(example)\n",
    "print(\"Image valide\" if validity else \"Image invalide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-11T09:20:29.374015Z",
     "iopub.status.busy": "2026-01-11T09:20:29.373687Z",
     "iopub.status.idle": "2026-01-11T09:56:28.751665Z",
     "shell.execute_reply": "2026-01-11T09:56:28.750620Z",
     "shell.execute_reply.started": "2026-01-11T09:20:29.373991Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "total = tree(base_folder, 2)\n",
    "print(f\"\\nNombre total de fichiers : {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T11:19:23.774461Z",
     "iopub.status.busy": "2026-01-11T11:19:23.774184Z",
     "iopub.status.idle": "2026-01-11T11:19:23.779579Z",
     "shell.execute_reply": "2026-01-11T11:19:23.778726Z",
     "shell.execute_reply.started": "2026-01-11T11:19:23.774440Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_annotated_images(json_path):\n",
    "    \"\"\"Charge lat/lon par file_name depuis train_mini_json.\"\"\"\n",
    "    annotated = {}\n",
    "    with open(json_path, 'rb') as f:\n",
    "        parser = ijson.items(f, 'images.item')\n",
    "        for img in parser:\n",
    "            filename = img.get('file_name', '')\n",
    "            lat = float(img.get('latitude')) if img.get('latitude') is not None else 0.0\n",
    "            lon = float(img.get('longitude')) if img.get('longitude') is not None else 0.0\n",
    "            annotated[filename] = (lat, lon)\n",
    "    return annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T11:19:31.985291Z",
     "iopub.status.busy": "2026-01-11T11:19:31.984870Z",
     "iopub.status.idle": "2026-01-11T11:19:31.993632Z",
     "shell.execute_reply": "2026-01-11T11:19:31.992596Z",
     "shell.execute_reply.started": "2026-01-11T11:19:31.985261Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_stats(full_taxa_map, full_geo_db, species_encountered):\n",
    "    \"\"\"Calcule toutes stats.\"\"\"\n",
    "    ordre_count = Counter()\n",
    "    famille_count = Counter()\n",
    "    genre_count = Counter()\n",
    "    espece_count = Counter()\n",
    "    \n",
    "    for taxon_key, hier in full_taxa_map.items():\n",
    "        ordre_count[hier['ordre']] += 1\n",
    "        famille_count[hier['famille']] += 1\n",
    "        genre_count[hier['genre']] += 1\n",
    "        espece_count[hier['espece']] += 1\n",
    "    \n",
    "    homonyms_count = len([s for s in species_encountered if len(species_encountered[s]) > 1])\n",
    "    homonyms_dirs = sum(len(species_encountered[s]) for s in species_encountered if len(species_encountered[s]) > 1)\n",
    "    \n",
    "    taxon_geo_counts = {str(k): len(v) for k, v in full_geo_db.items()}\n",
    "    geo_taxa = len(full_geo_db)\n",
    "    total_taxa = len(full_taxa_map)\n",
    "    multi_geo = sum(1 for c in taxon_geo_counts.values() if c > 1)\n",
    "    \n",
    "    return {\n",
    "        'total_dirs': len(species_encountered),\n",
    "        'unique_taxa': total_taxa,\n",
    "        'geo_coverage': geo_taxa / total_taxa if total_taxa else 0,\n",
    "        'homonyms': {'names': homonyms_count, 'dirs': homonyms_dirs},\n",
    "        'hierarchy': {\n",
    "            'ordres': len(ordre_count), 'familles': len(famille_count), 'genres': len(genre_count)\n",
    "        },\n",
    "        'taxon_geo_counts': taxon_geo_counts\n",
    "    }, ordre_count, famille_count, genre_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2026-01-11T11:31:04.266165Z",
     "iopub.status.busy": "2026-01-11T11:31:04.265370Z",
     "iopub.status.idle": "2026-01-11T11:31:10.026437Z",
     "shell.execute_reply": "2026-01-11T11:31:10.025528Z",
     "shell.execute_reply.started": "2026-01-11T11:31:04.266136Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "annotated_images = load_annotated_images(train_mini_json)\n",
    "species_encountered, unparsed_dirs = parse_taxonomy_folders(train_mini_folder)\n",
    "full_taxa_map, full_geo_db = build_taxa_maps(species_encountered, annotated_images, train_mini_folder)\n",
    "stats, ordre_count, famille_count, genre_count = compute_stats(full_taxa_map, full_geo_db, species_encountered)\n",
    "\n",
    "print(f\"✅ {stats['unique_taxa']} taxons (sur {stats['total_dirs']} dossiers)\")\n",
    "print(f\"Homonymes: {stats['homonyms']['names']} noms → {stats['homonyms']['dirs']} dossiers\")\n",
    "print(f\"Hiérarchie: {stats['hierarchy']['ordres']} ordres, {stats['hierarchy']['familles']} familles, {stats['hierarchy']['genres']} genres\")\n",
    "print(f\"Géo: {len(full_geo_db)}/{len(full_taxa_map)} ({stats['geo_coverage']*100:.1f}%)\")\n",
    "print(f\"Non parsés: {len(unparsed_dirs)}\")\n",
    "\n",
    "save_hierarchy_map(full_taxa_map, full_geo_db, stats, hierarchy_map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:29:14.223558Z",
     "iopub.status.busy": "2026-01-11T14:29:14.223257Z",
     "iopub.status.idle": "2026-01-11T14:29:14.233873Z",
     "shell.execute_reply": "2026-01-11T14:29:14.232695Z",
     "shell.execute_reply.started": "2026-01-11T14:29:14.223536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RobustImageFolder(Dataset):\n",
    "    \"\"\"ImageFolder skip corrompus.\"\"\"\n",
    "    def __init__(self, root, transform=None, corrupt_files=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.corrupt_files = set(corrupt_files or [])\n",
    "        \n",
    "        self.classes, self.class_to_idx = self.find_classes(self.root)\n",
    "        self.samples = self.make_dataset(self.root, self.class_to_idx)\n",
    "        \n",
    "        self.valid_indices = []\n",
    "        for i, (path, _) in enumerate(self.samples):\n",
    "            if os.path.relpath(path, self.root) not in self.corrupt_files:\n",
    "                self.valid_indices.append(i)\n",
    "    \n",
    "    def find_classes(self, directory):\n",
    "        \"\"\"Trouve classes (dossiers).\"\"\"\n",
    "        classes = [d.name for d in os.scandir(directory) if d.is_dir()]\n",
    "        classes.sort()\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "    \n",
    "    def make_dataset(self, directory, class_to_idx):\n",
    "        \"\"\"Construit samples comme ImageFolder.\"\"\"\n",
    "        samples = []\n",
    "        for target_class in sorted(self.class_to_idx.keys()):\n",
    "            class_index = self.class_to_idx[target_class]\n",
    "            target_dir = os.path.join(directory, target_class)\n",
    "            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
    "                for fname in sorted(fnames):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    item = (path, class_index)\n",
    "                    samples.append(item)\n",
    "        return samples\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[self.valid_indices[index]]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:10:31.060279Z",
     "iopub.status.busy": "2026-01-11T14:10:31.060006Z",
     "iopub.status.idle": "2026-01-11T14:19:58.673901Z",
     "shell.execute_reply": "2026-01-11T14:19:58.672672Z",
     "shell.execute_reply.started": "2026-01-11T14:10:31.060263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "corrupted_train, train_log = scan_corrupted_images('/kaggle/input/inaturalist-insects/train_mini/train_mini', max_workers=4)\n",
    "corrupted_val, val_log = scan_corrupted_images('/kaggle/input/inaturalist-insects/val/val', max_workers=4)\n",
    "\n",
    "with open(train_log) as f:\n",
    "    print(\"\\nCORROMPUS TRAIN:\\n\", f.read()[:500] + \"...\" if os.path.getsize(train_log) > 500 else f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:29:21.524378Z",
     "iopub.status.busy": "2026-01-11T14:29:21.524063Z",
     "iopub.status.idle": "2026-01-11T14:29:21.534947Z",
     "shell.execute_reply": "2026-01-11T14:29:21.534373Z",
     "shell.execute_reply.started": "2026-01-11T14:29:21.524359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "corrupt_train = []\n",
    "with open('/kaggle/working/corrupted_train_mini.txt') as f:\n",
    "    for line in f:\n",
    "        if line.strip() and not line.startswith('#'):\n",
    "            corrupt_train.append(line.strip())\n",
    "\n",
    "corrupt_val = []\n",
    "with open('/kaggle/working/corrupted_val.txt') as f:\n",
    "    for line in f:\n",
    "        if line.strip() and not line.startswith('#'):\n",
    "            corrupt_val.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:29:24.768465Z",
     "iopub.status.busy": "2026-01-11T14:29:24.768169Z",
     "iopub.status.idle": "2026-01-11T14:30:36.718824Z",
     "shell.execute_reply": "2026-01-11T14:30:36.717720Z",
     "shell.execute_reply.started": "2026-01-11T14:29:24.768449Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Datasets FULL (sans skip)\n",
    "train_dataset_full = RobustImageFolder(train_mini_folder)\n",
    "val_dataset_full = RobustImageFolder(val_folder)\n",
    "\n",
    "# Datasets CLEAN\n",
    "train_dataset = RobustImageFolder(train_mini_folder, train_transforms, corrupt_train)\n",
    "val_dataset = RobustImageFolder(val_folder, val_transforms, corrupt_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'Train: {len(train_dataset_full)} → {len(train_dataset)}')\n",
    "print(f'Val: {len(val_dataset_full)} → {len(val_dataset)}')\n",
    "print(f'Classes: {len(train_dataset.classes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:32:18.908926Z",
     "iopub.status.busy": "2026-01-11T14:32:18.908611Z",
     "iopub.status.idle": "2026-01-11T14:32:26.633926Z",
     "shell.execute_reply": "2026-01-11T14:32:26.632914Z",
     "shell.execute_reply.started": "2026-01-11T14:32:18.908908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class HierarchicalInsectDataset(Dataset):\n",
    "    def __init__(self, root_dir, hierarchy_map, transform=None, corrupt_files=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.hierarchy_map = hierarchy_map\n",
    "        self.transform = transform\n",
    "        \n",
    "        # ImageFolder interne\n",
    "        self.inner_dataset = datasets.ImageFolder(root_dir)\n",
    "        \n",
    "        # Filtre corrompus + hiérarchie\n",
    "        self.valid_indices = []\n",
    "        for i in range(len(self.inner_dataset)):\n",
    "            class_idx = self.inner_dataset.targets[i]\n",
    "            if class_idx in self.hierarchy_map:\n",
    "                # Skip si basename dans corrupt_files\n",
    "                path = self.inner_dataset.samples[i][0]\n",
    "                if os.path.basename(path) not in (corrupt_files or []):\n",
    "                    self.valid_indices.append(i)\n",
    "        \n",
    "        print(f\"Dataset {root_dir}: {len(self.inner_dataset)} → {len(self.valid_indices)} valides\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.valid_indices[idx]\n",
    "        img, class_idx = self.inner_dataset[real_idx]\n",
    "        \n",
    "        # Labels hiérarchiques [ordre_id, famille_id, genre_id, espece_id]\n",
    "        hier_labels = torch.tensor(self.hierarchy_map[class_idx])\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, hier_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "# === USAGE ===\n",
    "data_dir = '/kaggle/input/inaturalist-insects/'\n",
    "train_mini_folder = os.path.join(data_dir, 'train_mini/train_mini')\n",
    "val_folder = os.path.join(data_dir, 'val/val')\n",
    "\n",
    "# Datasets hiérarchiques\n",
    "train_dataset = HierarchicalInsectDataset(\n",
    "    train_mini_folder, \n",
    "    final_hierarchy, \n",
    "    transform=train_transforms,\n",
    "    corrupt_files=[os.path.basename(p) for p in corrupt_train]  # Seulement basenames\n",
    ")\n",
    "\n",
    "val_dataset = HierarchicalInsectDataset(\n",
    "    val_folder, \n",
    "    final_hierarchy, \n",
    "    transform=val_transforms,\n",
    "    corrupt_files=[os.path.basename(p) for p in corrupt_val]\n",
    ")\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"✅ Datasets hiérarchiques:\")\n",
    "print(f\"  Train: {len(train_dataset)} images, 2526 classes\")\n",
    "print(f\"  Val: {len(val_dataset)} images\")\n",
    "print(f\"  Labels: ordre/famille/genre/espece [device={device}]\")\n",
    "\n",
    "# Test 1 batch\n",
    "img, labels = next(iter(train_loader))\n",
    "print(f\"Batch shape: {img.shape}, labels shape: {labels.shape}\")  # [32,3,224,224], [32,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:39:06.995036Z",
     "iopub.status.busy": "2026-01-11T14:39:06.994708Z",
     "iopub.status.idle": "2026-01-11T14:39:08.934060Z",
     "shell.execute_reply": "2026-01-11T14:39:08.933051Z",
     "shell.execute_reply.started": "2026-01-11T14:39:06.995017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import json\n",
    "\n",
    "class HierarchicalMobileNetV3(nn.Module):\n",
    "    \"\"\"MobileNetV3 avec outputs séparés par niveau.\"\"\"\n",
    "    def __init__(self, num_ordre=17, num_famille=190, num_genre=1472, num_espece=2526):\n",
    "        super().__init__()\n",
    "        backbone = models.mobilenet_v3_large(weights='IMAGENET1K_V1')\n",
    "        self.features = backbone.features\n",
    "        self.avgpool = backbone.avgpool\n",
    "        \n",
    "        self.fc_shared = nn.Sequential(\n",
    "            nn.Linear(960, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Heads indépendants (FIX: pas cumulatif)\n",
    "        self.head_ordre = nn.Linear(512, num_ordre)\n",
    "        self.head_famille = nn.Linear(512, num_famille)\n",
    "        self.head_genre = nn.Linear(512, num_genre)\n",
    "        self.head_espece = nn.Linear(512, num_espece)\n",
    "    \n",
    "    def forward(self, x, return_probs=False):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        feats = torch.flatten(x, 1)\n",
    "        shared = self.fc_shared(feats)\n",
    "        \n",
    "        ordre = self.head_ordre(shared)\n",
    "        famille = self.head_famille(shared)\n",
    "        genre = self.head_genre(shared)\n",
    "        espece = self.head_espece(shared)\n",
    "        \n",
    "        if return_probs:\n",
    "            return (F.softmax(ordre, dim=1), F.softmax(famille, dim=1), \n",
    "                   F.softmax(genre, dim=1), F.softmax(espece, dim=1))\n",
    "        \n",
    "        # Stack [B, 4, max_classes] → pad à max\n",
    "        max_classes = 2526\n",
    "        preds = torch.zeros(x.size(0), 4, max_classes, device=x.device)\n",
    "        preds[:, 0, :ordre.size(1)] = ordre\n",
    "        preds[:, 1, :famille.size(1)] = famille\n",
    "        preds[:, 2, :genre.size(1)] = genre\n",
    "        preds[:, 3, :espece.size(1)] = espece\n",
    "        \n",
    "        return preds  # [B,4,2526]\n",
    "\n",
    "# === STATS ===\n",
    "with open('hierarchy_labels.json') as f:\n",
    "    stats = json.load(f)['stats']\n",
    "num_ordre = stats['ordres']      # 17\n",
    "num_famille = stats['familles']  # 190\n",
    "num_genre = stats['genres']     # 1472\n",
    "num_espece = stats['total_classes']  # 2526\n",
    "\n",
    "# Model\n",
    "model = HierarchicalMobileNetV3(num_ordre, num_famille, num_genre, num_espece).to(device)\n",
    "\n",
    "class HierarchicalLoss(nn.Module):\n",
    "    def __init__(self, num_classes_per_level):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.weights = torch.tensor([1.0, 2.0, 5.0, 10.0])\n",
    "        self.num_classes = num_classes_per_level  # [17,190,1472,2526]\n",
    "    \n",
    "    def forward(self, preds, targets):\n",
    "        loss = 0\n",
    "        for i in range(4):\n",
    "            mask = torch.arange(self.num_classes[i], device=preds.device)\n",
    "            lvl_pred = preds[:, i, mask]\n",
    "            lvl_loss = self.ce(lvl_pred, targets[:, i])\n",
    "            loss += self.weights[i] * lvl_loss\n",
    "        return loss / self.weights.sum()\n",
    "\n",
    "criterion = HierarchicalLoss([num_ordre, num_famille, num_genre, num_espece])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "print(f\"Model: {sum(p.numel() for p in model.parameters()):,} params\")\n",
    "print(f\"✅ GPU: {next(model.parameters()).device}\")\n",
    "\n",
    "# TEST FIX\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch_img, batch_labels = next(iter(train_loader))\n",
    "    preds = model(batch_img.to(device))  # [32,4,2526]\n",
    "    loss = criterion(preds, batch_labels.to(device))\n",
    "    print(f\"✅ Test OK: preds={preds.shape}, loss={loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:39:26.456563Z",
     "iopub.status.busy": "2026-01-11T14:39:26.456230Z",
     "iopub.status.idle": "2026-01-11T14:39:26.463744Z",
     "shell.execute_reply": "2026-01-11T14:39:26.462600Z",
     "shell.execute_reply.started": "2026-01-11T14:39:26.456535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class HierarchicalLoss(nn.Module):\n",
    "    def __init__(self, weights=[1.0, 1.5, 2.0, 3.0]):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.weights = weights\n",
    "    \n",
    "    def forward(self, preds, targets):\n",
    "        ordre_p, fam_p, genre_p, esp_p = preds\n",
    "        ordre_t, fam_t, genre_t, esp_t = targets[:, 0], targets[:, 1], targets[:, 2], targets[:, 3]\n",
    "        \n",
    "        loss0 = self.ce(ordre_p, ordre_t)\n",
    "        loss1 = self.ce(fam_p, fam_t)\n",
    "        loss2 = self.ce(genre_p, genre_t)\n",
    "        loss3 = self.ce(esp_p, esp_t)\n",
    "        \n",
    "        return sum(w * l for w, l in zip(self.weights, [loss0, loss1, loss2, loss3]))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8808792,
     "sourceId": 13841637,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
