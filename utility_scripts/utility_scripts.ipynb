{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5b86c6",
   "metadata": {},
   "source": [
    "# Utility Scripts\n",
    "\n",
    "Ce notebook contient l'int√©gralit√© du code des scripts utilitaires pour le projet dataset insectes.\n",
    "\n",
    "## tree.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3700388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def tree(\n",
    "    path: str | Path,\n",
    "    indent: int = 0,\n",
    "    tree_file: str | Path = \"tree.txt\",\n",
    "    _file=None,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Parcourt r√©cursivement l'arborescence du dossier pass√© en argument.\n",
    "    Utile pour v√©rifier le bon transfert des fichiers, en utilisant un programme\n",
    "    permettant la comparaison de l'arborescence export√©e avec celle ex√©cut√©e\n",
    "    localement avant l'upload du dataset.\n",
    "    \n",
    "    Note : Ce programme n'a pas pour but de v√©rifier l'int√©grit√© des images,\n",
    "    ce qui se r√©v√©lera √™tre un vrai probl√®me par la suite. En effet, avec les\n",
    "    interruptions d'Internet, certaines images ont √©t√© corrompues durant le\n",
    "    transfert, et il m'a fallu longtemps avant de m'en rendre compte.\n",
    "    \n",
    "    Args:\n",
    "        path: Chemin du dossier √† analyser\n",
    "        indent: Indentation pour l'affichage (interne)\n",
    "        tree_file: Fichier o√π √©crire l'arborescence\n",
    "        _file: Handle de fichier (interne)\n",
    "    \n",
    "    Returns:\n",
    "        Nombre total de fichiers dans l'arborescence\n",
    "    \"\"\"\n",
    "    \n",
    "    path = Path(path)\n",
    "    file_count = 0\n",
    "    subfolder_counts = []\n",
    "\n",
    "    close_file = False\n",
    "    if _file is None:\n",
    "        _file = open(tree_file, \"w\", encoding=\"utf-8\")\n",
    "        close_file = True\n",
    "\n",
    "    try:\n",
    "        entries = list(path.iterdir())\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Chemin introuvable : {path}\")\n",
    "    except PermissionError:\n",
    "        return 0\n",
    "\n",
    "    files = [e for e in entries if e.is_file()]\n",
    "    dirs = [e for e in entries if e.is_dir()]\n",
    "\n",
    "    file_count = len(files)\n",
    "\n",
    "    for d in sorted(dirs, key=lambda p: p.name.lower()):\n",
    "        sub_count = tree(\n",
    "            d,\n",
    "            indent=indent + 4,\n",
    "            tree_file=tree_file,\n",
    "            _file=_file,\n",
    "        )\n",
    "        subfolder_counts.append((d, sub_count))\n",
    "\n",
    "    line = \" \" * indent + f\"{path.name}/ [{file_count} fichiers]\\n\"\n",
    "    if _file is not None:\n",
    "        _file.write(line)\n",
    "\n",
    "    for subfolder, sub_count in subfolder_counts:\n",
    "        sub_line = \" \" * (indent + 2) + f\"{subfolder.name}/ [{sub_count} fichiers]\\n\"\n",
    "        if _file is not None:\n",
    "            _file.write(sub_line)\n",
    "\n",
    "    total_files = file_count + sum(count for _, count in subfolder_counts)\n",
    "\n",
    "    if close_file and _file is not None:\n",
    "        _file.close()\n",
    "\n",
    "    return total_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ba789",
   "metadata": {},
   "source": [
    "## taxonomy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad7679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_taxonomy(filepath):\n",
    "    \"\"\"\n",
    "    Extrait les informations taxonomiques du chemin d'une image.\n",
    "    \n",
    "    Cette fonction extrait le nom du dossier parent de l'image, car le nom de\n",
    "    l'image en elle-m√™me ne contient pas d'informations taxonomiques.\n",
    "    \n",
    "    Exemple de chemin :\n",
    "    train/train/00980_Animalia_Arthropoda_Insecta_Lepidoptera_Erebidae_Arctia_virginalis/464f3a34-4c04-4eb3-afa2-6cb7444c3fa3.jpg\n",
    "    \n",
    "    Taxon folder: 00980_Animalia_Arthropoda_Insecta_Lepidoptera_Erebidae_Arctia_virginalis\n",
    "    \n",
    "    Les informations sont s√©par√©es par '_'. Les √©l√©ments 0 (ID), 1, 2, 3 (constantes)\n",
    "    ne nous int√©ressent pas. Nous cr√©ons un dictionnaire avec :\n",
    "    - ordre : √©l√©ment 4 (ex: Lepidoptera)\n",
    "    - famille : √©l√©ment 5 (ex: Erebidae)\n",
    "    - genre : √©l√©ment 6 (ex: Arctia)\n",
    "    - espece : √©l√©ment 7 (ex: virginalis)\n",
    "    \n",
    "    Args:\n",
    "        filepath: Chemin complet de l'image\n",
    "        \n",
    "    Returns:\n",
    "        Dictionnaire avec les cl√©s 'ordre', 'famille', 'genre', 'espece' ou None si √©chec\n",
    "    \"\"\"\n",
    "    match = re.search(r'([^/]+)/[^/]+\\.jpg$', filepath)\n",
    "    if not match:\n",
    "        print(f\"Regex ne correspond pas : {filepath}\")\n",
    "        return None\n",
    "    folder = match.group(1)\n",
    "    taxonomy_path = folder.split('_')\n",
    "    if len(taxonomy_path) >= 7:\n",
    "        return {\n",
    "            'ordre': taxonomy_path[4],  \n",
    "            'famille': taxonomy_path[5],\n",
    "            'genre': taxonomy_path[6],\n",
    "            'espece': taxonomy_path[7]\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def parse_taxonomy_folders(folder_path):\n",
    "    \"\"\"\n",
    "    Parse la taxonomie depuis les noms des dossiers dans un r√©pertoire.\n",
    "    \n",
    "    Parcourt r√©cursivement le dossier donn√© et applique parse_taxonomy √† chaque\n",
    "    sous-dossier (en simulant un fichier image.jpg virtuel pour extraire la taxonomie).\n",
    "    Regroupe les esp√®ces rencontr√©es et liste les dossiers non parsables.\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Chemin du dossier racine √† analyser\n",
    "        \n",
    "    Returns:\n",
    "        Tuple (species_encountered, unparsed):\n",
    "        - species_encountered: dict[str, list] o√π cl√© = nom d'esp√®ce, valeur = liste de (dossier, hi√©rarchie)\n",
    "        - unparsed: list des chemins de dossiers non parsables\n",
    "    \"\"\"\n",
    "    unparsed = []\n",
    "    species_encountered = defaultdict(list)\n",
    "    for root, dirs_, files in os.walk(folder_path):\n",
    "        for d in dirs_:\n",
    "            hier = parse_taxonomy(d + \"/image.jpg\")\n",
    "            if hier:\n",
    "                species_name = hier['espece']\n",
    "                species_encountered[species_name].append((d, hier))\n",
    "            else:\n",
    "                unparsed.append(os.path.join(root, d))\n",
    "    return species_encountered, unparsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09925484",
   "metadata": {},
   "source": [
    "## mapping.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51848fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_taxa_maps(species_encountered, annotated_images, train_mini_folder):\n",
    "    \"\"\"\n",
    "    Construit les mappings taxonomiques et g√©ographiques complets.\n",
    "    \n",
    "    √Ä partir des esp√®ces rencontr√©es et des annotations g√©ographiques,\n",
    "    cr√©e un mapping de taxons (cl√© = tuple (ordre, famille, genre, esp√®ce))\n",
    "    vers la hi√©rarchie, et une base de donn√©es g√©ographique associant\n",
    "    chaque taxon √† ses coordonn√©es (lat, lon) issues des images annot√©es.\n",
    "    \n",
    "    Args:\n",
    "        species_encountered: dict[str, list] des esp√®ces (de parse_taxonomy_folders)\n",
    "        annotated_images: dict[str, tuple] chemin relatif -> (lat, lon)\n",
    "        train_mini_folder: chemin du dossier train_mini\n",
    "        \n",
    "    Returns:\n",
    "        Tuple (full_taxa_map, full_geo_db):\n",
    "        - full_taxa_map: dict[tuple, dict] taxon -> hi√©rarchie\n",
    "        - full_geo_db: dict[tuple, list] taxon -> liste de [lat, lon]\n",
    "    \"\"\"\n",
    "    full_taxa_map = {}\n",
    "    full_geo_db = defaultdict(list)\n",
    "    \n",
    "    for species_name, occurrences in species_encountered.items():\n",
    "        for d, hier in occurrences:\n",
    "            taxon_key = (hier['ordre'], hier['famille'], hier['genre'], species_name)\n",
    "            if taxon_key not in full_taxa_map:\n",
    "                full_taxa_map[taxon_key] = hier\n",
    "            \n",
    "            taxon_folder = os.path.join(train_mini_folder, d)\n",
    "            if os.path.exists(taxon_folder):\n",
    "                _, _, files = next(os.walk(taxon_folder))\n",
    "                seen_rel_paths = set()\n",
    "                for f in files:\n",
    "                    rel_path = f\"train_mini/{d}/{f}\"\n",
    "                    if rel_path in annotated_images and rel_path not in seen_rel_paths:\n",
    "                        lat, lon = annotated_images[rel_path]\n",
    "                        if lat != 0.0 and lon != 0.0:\n",
    "                            full_geo_db[taxon_key].append([lat, lon])\n",
    "                            seen_rel_paths.add(rel_path)\n",
    "    \n",
    "    return full_taxa_map, full_geo_db\n",
    "\n",
    "def save_hierarchy_map(full_taxa_map, full_geo_db, stats, output_file):\n",
    "    \"\"\"\n",
    "    Sauvegarde la hi√©rarchie taxonomique et les donn√©es g√©ographiques dans un fichier JSON.\n",
    "    \n",
    "    S√©rialise les mappings et statistiques pour une utilisation ult√©rieure,\n",
    "    par exemple dans les datasets hi√©rarchiques.\n",
    "    \n",
    "    Args:\n",
    "        full_taxa_map: dict[tuple, dict] des taxons\n",
    "        full_geo_db: dict[tuple, list] des coordonn√©es\n",
    "        stats: dict des statistiques\n",
    "        output_file: chemin du fichier JSON de sortie\n",
    "    \"\"\"\n",
    "    full_geo_serializable = {str(k): [[float(c[0]), float(c[1])] for c in v] \n",
    "                            for k, v in full_geo_db.items()}\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'full_taxa_map': {str(k): v for k, v in full_taxa_map.items()},\n",
    "            'full_geo_db': full_geo_serializable,\n",
    "            'stats': stats\n",
    "        }, f, indent=2)\n",
    "    print(f\"üíæ Sauvegard√©: {len(full_taxa_map)} taxons dans {output_file}\")\n",
    "\n",
    "def build_hierarchy_labels(data_dir, hierarchy_map_file):\n",
    "    \"\"\"\n",
    "    Construit le mapping des indices ImageFolder vers les labels hi√©rarchiques.\n",
    "    \n",
    "    √Ä partir des classes ImageFolder et de la hi√©rarchie sauvegard√©e,\n",
    "    cr√©e un dictionnaire associant chaque index de classe √† une liste\n",
    "    [ordre_id, famille_id, genre_id, espece_id] pour l'entra√Ænement hi√©rarchique.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: r√©pertoire des donn√©es (contient train_mini)\n",
    "        hierarchy_map_file: fichier JSON de la hi√©rarchie\n",
    "        \n",
    "    Returns:\n",
    "        dict[int, list]: mapping index -> labels hi√©rarchiques\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Scan dossiers ‚Üí classes ImageFolder\n",
    "    train_path = os.path.join(data_dir, 'train_mini/train_mini')\n",
    "    class_names = sorted([d for d in os.listdir(train_path) \n",
    "                         if os.path.isdir(os.path.join(train_path, d))])\n",
    "    class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "    print(f\"Classes: {len(class_to_idx)} (scan {train_path})\")\n",
    "    \n",
    "    # 2. Hi√©rarchie depuis JSON\n",
    "    with open(hierarchy_map_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    full_taxa_map_str = data['full_taxa_map']\n",
    "    unique_ordres = set()\n",
    "    unique_familles = set()\n",
    "    unique_genres = set()\n",
    "    \n",
    "    for taxon_str, hier in full_taxa_map_str.items():\n",
    "        parts = taxon_str.strip(\"('\").strip(\"')\").split(\"', '\")\n",
    "        if len(parts) == 4:\n",
    "            unique_ordres.add(parts[0])\n",
    "            unique_familles.add(parts[1])\n",
    "            unique_genres.add(parts[2])\n",
    "    \n",
    "    ordre_to_id = {name: i for i, name in enumerate(sorted(unique_ordres))}\n",
    "    famille_to_id = {name: i for i, name in enumerate(sorted(unique_familles))}\n",
    "    genre_to_id = {name: i for i, name in enumerate(sorted(unique_genres))}\n",
    "    \n",
    "    print(f\"Hi√©rarchie: {len(ordre_to_id)} ordres, {len(famille_to_id)} fam., {len(genre_to_id)} genres\")\n",
    "    \n",
    "    # 3. Mapping\n",
    "    final_hierarchy = {}\n",
    "    mapped = 0\n",
    "    \n",
    "    for class_name, class_idx in class_to_idx.items():\n",
    "        parts = class_name.split('_')\n",
    "        if len(parts) >= 4:\n",
    "            ordre, famille, genre, espece = parts[-4:]\n",
    "            \n",
    "            taxon_key_str = f\"('{ordre}', '{famille}', '{genre}', '{espece}')\"\n",
    "            \n",
    "            if taxon_key_str in full_taxa_map_str:\n",
    "                hier = full_taxa_map_str[taxon_key_str]\n",
    "                final_hierarchy[class_idx] = [\n",
    "                    ordre_to_id[hier['ordre']],\n",
    "                    famille_to_id[hier['famille']],\n",
    "                    genre_to_id[hier['genre']],\n",
    "                    class_idx\n",
    "                ]\n",
    "                mapped += 1\n",
    "            else:\n",
    "                if mapped == 0:\n",
    "                    print(f\"DEBUG: cl√© g√©n√©r√©e '{taxon_key_str}' non trouv√©e.\")\n",
    "                    print(f\"Exemple cl√© JSON: {list(full_taxa_map_str.keys())[0]}\")\n",
    "    \n",
    "    print(f\"‚úÖ {mapped}/{len(class_to_idx)} mapp√©es\")\n",
    "    \n",
    "    # Sauvegarde\n",
    "    with open('hierarchy_labels.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'class_to_idx': class_to_idx,\n",
    "            'final_hierarchy': final_hierarchy,  # {0: [2, 45, 123, 0], 1: [3, 46, 124, 1], ...}\n",
    "            'id_to_name': {\n",
    "                'ordre': {i: name for name, i in ordre_to_id.items()},\n",
    "                'famille': {i: name for name, i in famille_to_id.items()},\n",
    "                'genre': {i: name for name, i in genre_to_id.items()}\n",
    "            },\n",
    "            'stats': {\n",
    "                'ordres': len(ordre_to_id),\n",
    "                'familles': len(famille_to_id),\n",
    "                'genres': len(genre_to_id),\n",
    "                'total_classes': len(class_to_idx),\n",
    "                'mapped': mapped\n",
    "            }\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(\"üíæ hierarchy_labels.json pr√™t pour training\")\n",
    "    return final_hierarchy\n",
    "\n",
    "# === USAGE ===\n",
    "data_dir = '/kaggle/input/inaturalist-insects/'\n",
    "hierarchy_map_file = '/kaggle/working/hierarchy_map.json'\n",
    "final_hierarchy = build_hierarchy_labels(data_dir, hierarchy_map_file)\n",
    "\n",
    "print(\"\\nExemples:\")\n",
    "for idx in range(25):\n",
    "    labels = final_hierarchy.get(idx)\n",
    "    print(f\"Class {idx}: {labels}\") # [ordre_id, famille_id, genre_id, espece_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240cc28",
   "metadata": {},
   "source": [
    "## corruption_scan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from PIL import Image\n",
    "\n",
    "def verify_image_validity(full_path):\n",
    "    \"\"\"\n",
    "    V√©rifie l'int√©grit√© d'une image.\n",
    "    \n",
    "    Cette fonction a √©t√© ajout√©e apr√®s un premier entra√Ænement complet, car nous\n",
    "    exp√©rimentions des erreurs difficiles √† trouver : la corruption d'images,\n",
    "    qui ayant servi √† entra√Æner le mod√®le, introduisait des erreurs dans les\n",
    "    pr√©dictions (par exemple, ordre et famille corrects, mais genre et esp√®ce\n",
    "    n'existant pas sous cette famille).\n",
    "    \n",
    "    Nous ne pouvons pas √©liminer ces images du dataset sur Kaggle car le dossier\n",
    "    d'input est en lecture seule. Pour contourner le probl√®me, nous √©crivons\n",
    "    dans un fichier la liste des fichiers corrompus √† l'avance, puis lors de\n",
    "    l'entra√Ænement final, nous ajoutons une v√©rification pour ne pas traiter\n",
    "    les images dont le chemin appara√Æt dans cette liste.\n",
    "    \n",
    "    Args:\n",
    "        full_path: Chemin complet de l'image √† v√©rifier\n",
    "        \n",
    "    Returns:\n",
    "        Tuple (bool, str): True si valide, False sinon, avec message d'erreur\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(full_path):\n",
    "            return False, \"FILE_MISSING\"\n",
    "        \n",
    "        with Image.open(full_path) as img:\n",
    "            img.verify()\n",
    "        \n",
    "        img = Image.open(full_path).convert('RGB')\n",
    "        img.thumbnail((64, 64))\n",
    "        img = img.resize((224, 224))\n",
    "        \n",
    "        return True, None\n",
    "    except Exception as e:\n",
    "        return False, str(type(e).__name__) + \": \" + str(e)[:50]\n",
    "    \n",
    "def scan_corrupted_images(root_folder, max_workers=4):\n",
    "    \"\"\"\n",
    "    Scanne un dossier pour d√©tecter les images corrompues.\n",
    "    \n",
    "    Parcourt r√©cursivement le dossier, v√©rifie chaque image JPG/JPEG\n",
    "    avec verify_image_validity, et sauvegarde la liste des fichiers\n",
    "    corrompus dans un fichier texte. Utilise un ThreadPool pour\n",
    "    parall√©liser les v√©rifications.\n",
    "    \n",
    "    Args:\n",
    "        root_folder: dossier racine √† scanner\n",
    "        max_workers: nombre de threads pour la parall√©lisation\n",
    "        \n",
    "    Returns:\n",
    "        Tuple (corrupted, log_file):\n",
    "        - corrupted: list des chemins relatifs des images corrompues\n",
    "        - log_file: chemin du fichier log des erreurs\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(root_folder):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(('.jpg', '.jpeg')):  # JPG seulement\n",
    "                full_path = os.path.join(root, f)\n",
    "                rel_path = os.path.relpath(full_path, root_folder)\n",
    "                image_paths.append(rel_path)\n",
    "    \n",
    "    total_files = len(image_paths)\n",
    "    print(f\"Scan {total_files} JPG dans {root_folder}\")\n",
    "    \n",
    "    if total_files == 0:\n",
    "        return [], f\"/kaggle/working/corrupted_{os.path.basename(root_folder)}.txt\"\n",
    "    \n",
    "    corrupted = []\n",
    "    error_types = defaultdict(int)\n",
    "    \n",
    "    # ThreadPool (fichiers I/O)\n",
    "    full_paths = [os.path.join(root_folder, p) for p in image_paths]\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(verify_image_validity, fp): fp for fp in full_paths}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            fp = futures[future]\n",
    "            is_valid, error = future.result()\n",
    "            \n",
    "            if not is_valid:\n",
    "                rel_path = os.path.relpath(fp, root_folder)\n",
    "                corrupted.append(rel_path)\n",
    "                error_types[error] += 1\n",
    "                if len(corrupted) < 10:  # 10 premiers\n",
    "                    print(f\"‚ùå {rel_path[:50]}: {error}\")\n",
    "    \n",
    "    # Stats erreurs\n",
    "    print(\"\\nTYPES D'ERREURS:\")\n",
    "    for err, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {err}: {count}\")\n",
    "    \n",
    "    # Sauvegarde\n",
    "    output_file = f\"/kaggle/working/corrupted_{os.path.basename(root_folder)}.txt\"\n",
    "    rate = len(corrupted) / total_files * 100\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(f\"# Corrompus: {len(corrupted)}/{total_files} ({rate:.1f}%)\\n\")\n",
    "        f.write(\"# Erreurs:\\n\")\n",
    "        for err, count in error_types.items():\n",
    "            f.write(f\"# {err}: {count}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        for path in corrupted:\n",
    "            f.write(path + '\\n')\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úÖ {len(corrupted)}/{total_files} ({rate:.1f}%) en {elapsed:.1f}s\")\n",
    "    \n",
    "    return corrupted, output_file"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
